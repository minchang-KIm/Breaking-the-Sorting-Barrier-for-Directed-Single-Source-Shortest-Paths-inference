# Research Proposal: HPC-Optimized SSSP with Multi-GPU NVLINK and Asynchronous Pipeline
## ì—°êµ¬ ì œì•ˆì„œ: Multi-GPU NVLINK ë° ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ì„ í™œìš©í•œ HPC ìµœì í™” SSSP

**Author:** [Your Name]
**Date:** 2025-11-17
**Target:** Winter Domestic Conference Paper
**Repository:** Breaking-the-Sorting-Barrier-for-Directed-Single-Source-Shortest-Paths-inference

---

## 1. Research Motivation and Objectives
## 1. ì—°êµ¬ ë™ê¸° ë° ëª©í‘œ

### 1.1 Background
### 1.1 ë°°ê²½

The recent breakthrough algorithm by Duan et al. (2025) achieves **O(m log^(2/3) n)** time complexity for directed single-source shortest paths (SSSP), breaking the O(m + n log n) sorting barrier that has stood since Dijkstra's algorithm (1959). However, the original paper focuses on theoretical complexity and sequential implementation, leaving significant opportunities for HPC optimization.

Duan ë“±(2025)ì˜ ìµœê·¼ íšê¸°ì ì¸ ì•Œê³ ë¦¬ì¦˜ì€ ë°©í–¥ ê·¸ë˜í”„ì˜ ë‹¨ì¼ ì¶œë°œì  ìµœë‹¨ ê²½ë¡œ(SSSP) ë¬¸ì œì—ì„œ **O(m log^(2/3) n)** ì‹œê°„ ë³µì¡ë„ë¥¼ ë‹¬ì„±í•˜ì—¬, Dijkstra ì•Œê³ ë¦¬ì¦˜(1959) ì´í›„ ìœ ì§€ë˜ì–´ ì˜¨ O(m + n log n) ì •ë ¬ ì¥ë²½ì„ ëŒíŒŒí–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì› ë…¼ë¬¸ì€ ì´ë¡ ì  ë³µì¡ë„ì™€ ìˆœì°¨ êµ¬í˜„ì— ì´ˆì ì„ ë§ì¶”ê³  ìˆì–´, HPC ìµœì í™”ë¥¼ ìœ„í•œ ìƒë‹¹í•œ ê¸°íšŒê°€ ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.

### 1.2 Research Gap
### 1.2 ì—°êµ¬ ê³µë°±

**Current State:**
- Sequential implementation available
- Basic OpenMP/MPI parallelization exists
- Single-GPU CUDA implementation with atomicMin optimization
- Limited scalability for billion-edge graphs

**í˜„ì¬ ìƒíƒœ:**
- ìˆœì°¨ êµ¬í˜„ ê°€ëŠ¥
- ê¸°ë³¸ OpenMP/MPI ë³‘ë ¬í™” ì¡´ì¬
- atomicMin ìµœì í™”ë¥¼ ì ìš©í•œ ë‹¨ì¼ GPU CUDA êµ¬í˜„
- ìˆ˜ì‹­ì–µ ê°„ì„  ê·¸ë˜í”„ì— ëŒ€í•œ ì œí•œì  í™•ì¥ì„±

**Missing:**
- Multi-GPU coordination with NVLINK high-bandwidth interconnect
- Overlapping computation and communication (asynchronous pipeline)
- Optimized graph partitioning for minimal edge-cut
- Comprehensive performance analysis on HPC clusters

**ë¶€ì¡±í•œ ë¶€ë¶„:**
- NVLINK ê³ ëŒ€ì—­í­ ì¸í„°ì»¤ë„¥íŠ¸ë¥¼ í™œìš©í•œ Multi-GPU ì¡°ì •
- ê³„ì‚°ê³¼ í†µì‹ ì„ ì¤‘ì²©í•˜ëŠ” ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸
- ìµœì†Œ ê°„ì„  ì ˆë‹¨ì„ ìœ„í•œ ìµœì í™”ëœ ê·¸ë˜í”„ ë¶„í• 
- HPC í´ëŸ¬ìŠ¤í„°ì—ì„œì˜ í¬ê´„ì  ì„±ëŠ¥ ë¶„ì„

### 1.3 Research Objectives
### 1.3 ì—°êµ¬ ëª©í‘œ

**Primary Objective:**
Propose and implement a **Multi-GPU Asynchronous Pipeline (MGAP)** optimization technique that:
1. Exploits NVLINK's 600GB/s bandwidth for direct GPU-to-GPU communication
2. Overlaps computation phases with inter-GPU data transfer
3. Reduces edge-cut through intelligent graph partitioning
4. Achieves **10-50x speedup** over sequential baseline on HPC systems

**ì£¼ìš” ëª©í‘œ:**
ë‹¤ìŒì„ ìˆ˜í–‰í•˜ëŠ” **Multi-GPU ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸(MGAP)** ìµœì í™” ê¸°ë²•ì„ ì œì•ˆí•˜ê³  êµ¬í˜„:
1. NVLINKì˜ 600GB/s ëŒ€ì—­í­ì„ í™œìš©í•œ ì§ì ‘ GPU-to-GPU í†µì‹ 
2. ê³„ì‚° ë‹¨ê³„ì™€ GPU ê°„ ë°ì´í„° ì „ì†¡ì„ ì¤‘ì²©
3. ì§€ëŠ¥ì  ê·¸ë˜í”„ ë¶„í• ì„ í†µí•œ ê°„ì„  ì ˆë‹¨ ê°ì†Œ
4. HPC ì‹œìŠ¤í…œì—ì„œ ìˆœì°¨ ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„ **10-50ë°° ì†ë„ í–¥ìƒ** ë‹¬ì„±

**Secondary Objectives:**
- Compare against classical algorithms (Dijkstra, Bellman-Ford)
- Analyze time/space complexity theoretically and empirically
- Measure communication volume and edge-cut metrics
- Provide ease-of-use analysis for adoption in production systems

**ë¶€ì°¨ ëª©í‘œ:**
- ê³ ì „ ì•Œê³ ë¦¬ì¦˜(Dijkstra, Bellman-Ford)ê³¼ ë¹„êµ
- ì´ë¡ ì  ë° ê²½í—˜ì ìœ¼ë¡œ ì‹œê°„/ê³µê°„ ë³µì¡ë„ ë¶„ì„
- í†µì‹ ëŸ‰ ë° ê°„ì„  ì ˆë‹¨ ë©”íŠ¸ë¦­ ì¸¡ì •
- í”„ë¡œë•ì…˜ ì‹œìŠ¤í…œ ë„ì…ì„ ìœ„í•œ ì‚¬ìš© ìš©ì´ì„± ë¶„ì„

---

## 2. Proposed Technique: Multi-GPU Asynchronous Pipeline (MGAP)
## 2. ì œì•ˆ ê¸°ë²•: Multi-GPU ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ (MGAP)

### 2.1 Core Innovation
### 2.1 í•µì‹¬ í˜ì‹ 

**Technique Name:** Multi-GPU Asynchronous Pipeline (MGAP) for SSSP
**ê¸°ë²• ëª…ì¹­:** SSSPë¥¼ ìœ„í•œ Multi-GPU ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ (MGAP)

**Key Components:**

1. **NVLINK-Accelerated Multi-GPU Coordination**
   - Direct P2P memory access between GPUs (bypass PCIe bottleneck)
   - Up to 600GB/s bandwidth vs 16GB/s PCIe Gen3
   - Expected: **3-5x communication speedup**

2. **Asynchronous Computation-Communication Pipeline**
   - CUDA streams for overlapping kernel execution and memory transfer
   - Triple-buffering strategy: compute(GPU_i) || transfer(GPU_j) || prepare(GPU_k)
   - Expected: **20-30% latency hiding improvement**

3. **METIS-Based Intelligent Graph Partitioning**
   - Minimize edge-cut using k-way partitioning
   - Balance vertex distribution across GPUs
   - Expected: **30-50% reduction in inter-GPU communication**

4. **Lock-Free Atomic Distance Updates**
   - Custom atomicMinDouble using CAS (Compare-And-Swap)
   - Eliminates mutex contention in parallel edge relaxation
   - Expected: **15-25% reduction in atomic operation overhead**

**ì£¼ìš” êµ¬ì„± ìš”ì†Œ:**

1. **NVLINK ê°€ì† Multi-GPU ì¡°ì •**
   - GPU ê°„ ì§ì ‘ P2P ë©”ëª¨ë¦¬ ì•¡ì„¸ìŠ¤ (PCIe ë³‘ëª© ìš°íšŒ)
   - PCIe Gen3 16GB/s ëŒ€ë¹„ ìµœëŒ€ 600GB/s ëŒ€ì—­í­
   - ì˜ˆìƒ: **í†µì‹  ì†ë„ 3-5ë°° í–¥ìƒ**

2. **ë¹„ë™ê¸° ê³„ì‚°-í†µì‹  íŒŒì´í”„ë¼ì¸**
   - ì»¤ë„ ì‹¤í–‰ê³¼ ë©”ëª¨ë¦¬ ì „ì†¡ì„ ì¤‘ì²©í•˜ëŠ” CUDA ìŠ¤íŠ¸ë¦¼
   - ì‚¼ì¤‘ ë²„í¼ë§ ì „ëµ: compute(GPU_i) || transfer(GPU_j) || prepare(GPU_k)
   - ì˜ˆìƒ: **ì§€ì—° ì‹œê°„ ì€ë‹‰ 20-30% ê°œì„ **

3. **METIS ê¸°ë°˜ ì§€ëŠ¥í˜• ê·¸ë˜í”„ ë¶„í• **
   - k-way ë¶„í• ì„ ì‚¬ìš©í•œ ê°„ì„  ì ˆë‹¨ ìµœì†Œí™”
   - GPU ê°„ ì •ì  ë¶„í¬ ê· í˜• ìœ ì§€
   - ì˜ˆìƒ: **GPU ê°„ í†µì‹  30-50% ê°ì†Œ**

4. **ë½-í”„ë¦¬ ì›ìì  ê±°ë¦¬ ì—…ë°ì´íŠ¸**
   - CAS(Compare-And-Swap)ë¥¼ ì‚¬ìš©í•œ ì»¤ìŠ¤í…€ atomicMinDouble
   - ë³‘ë ¬ ê°„ì„  ì™„í™”ì—ì„œ ë®¤í…ìŠ¤ ê²½í•© ì œê±°
   - ì˜ˆìƒ: **ì›ì ì—°ì‚° ì˜¤ë²„í—¤ë“œ 15-25% ê°ì†Œ**

### 2.2 Algorithm Design
### 2.2 ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„

**High-Level Workflow:**

```
Algorithm: Multi-GPU Asynchronous Pipeline SSSP (MGAP-SSSP)
Input: Graph G(V, E, w), source vertex s, GPU count k
Output: Shortest distances d[v] for all v âˆˆ V

1. Preprocessing Phase:
   a. Partition graph using METIS k-way partitioning
   b. Distribute partitions to k GPUs with balanced load
   c. Build CSR representation on each GPU
   d. Enable NVLINK P2P access between all GPU pairs

2. Initialization Phase:
   a. Set d[s] = 0, d[v] = âˆ for v â‰  s on all GPUs
   b. Create CUDA streams: compute_stream[k], transfer_stream[k]
   c. Allocate triple-buffers for boundary vertices

3. Iterative Relaxation Phase (Asynchronous Pipeline):
   for iteration = 1 to max_iterations:
       // Stage 1: Local relaxation (parallel across GPUs)
       for each GPU_i in parallel:
           Launch relax_edges_kernel on compute_stream[i]
           Process local edges using atomicMinDouble

       // Stage 2: Boundary exchange (overlapped with next iteration)
       for each GPU_i in parallel:
           Async copy boundary distances to neighbors via NVLINK
           Use transfer_stream[i] for non-blocking transfer

       // Stage 3: Global convergence check (Allreduce)
       changed = MPI_Allreduce(local_changed, MPI_LOR)
       if not changed:
           break

4. Path Reconstruction Phase:
   Gather predecessor information from all GPUs
   Reconstruct path from s to target vertex

Return: Distance array d[]
```

**Technical Details:**

- **Graph Partitioning:** METIS multilevel k-way partitioning with edge-cut minimization
- **Load Balancing:** Balance vertices Â±5% across GPUs
- **Communication Pattern:** Halo exchange for boundary vertices only
- **Synchronization:** Asynchronous barriers using CUDA events
- **Memory Management:** Unified memory with prefetching hints

---

## 3. Experimental Design
## 3. ì‹¤í—˜ ì„¤ê³„

### 3.1 Algorithms to Compare
### 3.1 ë¹„êµ ëŒ€ìƒ ì•Œê³ ë¦¬ì¦˜

| Algorithm | Time Complexity | Space Complexity | Implementation |
|-----------|----------------|------------------|----------------|
| Dijkstra (Sequential) | O((m+n) log n) | O(n) | CPU with binary heap |
| Bellman-Ford (Sequential) | O(nm) | O(n) | CPU iterative |
| Duan et al. (Sequential) | O(m log^(2/3) n) | O(n+m) | CPU baseline |
| Duan et al. (OpenMP) | O(m log^(2/3) n / p) | O(n+m) | Multi-core CPU |
| Duan et al. (Single-GPU) | O(m log^(2/3) n / p) | O(n+m) | CUDA baseline |
| **MGAP-SSSP (Proposed)** | **O(m log^(2/3) n / kp)** | **O(n+m)** | **Multi-GPU NVLINK** |

Where:
- m: number of edges
- n: number of vertices
- p: parallelism factor (GPU threads)
- k: number of GPUs

### 3.2 Datasets
### 3.2 ë°ì´í„°ì…‹

**Small Scale (Correctness Validation):**
1. **Simple Graph:** 4 vertices, 5 edges (unit test)
2. **Grid Graph:** 1,000 vertices, ~2,000 edges
3. **Random DAG:** 10,000 vertices, 50,000 edges

**Medium Scale (Performance Baseline):**
4. **Road Network (USA-small):** 100K vertices, 250K edges
5. **Social Network (Twitter-sample):** 500K vertices, 2M edges
6. **Random Sparse:** 1M vertices, 5M edges (avg degree 5)

**Large Scale (HPC Scalability):**
7. **Road Network (USA-full):** 24M vertices, 58M edges
8. **Web Graph (Google):** 875K vertices, 5.1M edges
9. **Synthetic Scale-Free:** 100M vertices, 1B edges (power-law distribution)

**Dataset Sources:**
- Road networks: DIMACS Challenge (9th, 10th)
- Social networks: Stanford SNAP
- Synthetic: Custom graph generator with configurable parameters

### 3.3 Evaluation Metrics
### 3.3 í‰ê°€ ë©”íŠ¸ë¦­

**Performance Metrics:**
1. **Execution Time (ms):** Wall-clock time from start to convergence
2. **Speedup:** T_sequential / T_parallel
3. **Throughput (MTEPS):** Million Traversed Edges Per Second
4. **Scalability:** Weak/Strong scaling curves

**Resource Metrics:**
5. **Memory Usage (GB):** Peak GPU/CPU memory consumption
6. **Memory Bandwidth (GB/s):** Effective utilization of NVLINK/PCIe
7. **GPU Utilization (%):** Kernel execution time / total time

**Communication Metrics:**
8. **Edge-Cut:** Number of edges crossing partition boundaries
9. **Communication Volume (MB):** Total data transferred between GPUs
10. **Communication Time (%):** Ratio of communication to total time
11. **Message Count:** Number of inter-GPU synchronization events

**Quality Metrics:**
12. **Correctness:** Distance error compared to sequential baseline (tolerance 1e-5)
13. **Path Recovery:** Verification of shortest path reconstruction

### 3.4 Hardware Configuration
### 3.4 í•˜ë“œì›¨ì–´ êµ¬ì„±

**HPC System Specification (Expected):**
- **GPUs:** 4Ã— NVIDIA A100 80GB with NVLINK (600GB/s per link)
- **CPU:** 2Ã— AMD EPYC 7742 64-core (128 cores total)
- **RAM:** 512GB DDR4-3200
- **Interconnect:** NVLINK 3.0 (GPU-GPU), PCIe Gen4 (CPU-GPU)
- **Storage:** 4TB NVMe SSD for dataset staging

**Software Stack:**
- **OS:** Ubuntu 22.04 LTS
- **CUDA:** 12.0 or higher
- **MPI:** OpenMPI 4.1 with CUDA-aware support
- **Compiler:** GCC 11.4, nvcc 12.0
- **Libraries:** METIS 5.1, OpenMP 4.5

---

## 4. Expected Results and Contributions
## 4. ì˜ˆìƒ ê²°ê³¼ ë° ê¸°ì—¬

### 4.1 Expected Performance Gains
### 4.1 ì˜ˆìƒ ì„±ëŠ¥ í–¥ìƒ

**Quantitative Targets:**

| Metric | Sequential Baseline | Single-GPU | **MGAP (4 GPUs)** | **Improvement** |
|--------|---------------------|------------|-------------------|-----------------|
| Execution Time (1M edges) | 1,200 ms | 45 ms | **12 ms** | **100Ã— faster** |
| Edge-Cut (%) | N/A | 100% | **35%** | **65% reduction** |
| Communication Volume | N/A | 500 MB | **180 MB** | **64% reduction** |
| Memory Efficiency | 100% | 85% | **78%** | **22% overhead** |
| Scalability (4â†’8 GPUs) | N/A | N/A | **1.7Ã— speedup** | **Linear trend** |

**Qualitative Contributions:**

1. **Breaking Theoretical Barriers in Practice:**
   - First HPC implementation of Duan et al.'s O(m log^(2/3) n) algorithm
   - Demonstrates practical viability of theoretical breakthrough
   - Bridges gap between algorithmic theory and high-performance computing

2. **Novel Parallelization Strategy:**
   - Asynchronous pipeline architecture for graph algorithms
   - Generalizable to other graph problems (BFS, PageRank, etc.)
   - Template for future GPU-accelerated divide-and-conquer algorithms

3. **Domestic Research Contribution:**
   - Advances South Korea's competitiveness in HPC and graph analytics
   - Provides open-source reference implementation for researchers
   - Applications in transportation networks, social media analysis, etc.

### 4.2 Paper Contributions
### 4.2 ë…¼ë¬¸ ê¸°ì—¬ë„

**Main Contributions:**

1. **Algorithm Design:**
   - Multi-GPU asynchronous pipeline technique (MGAP)
   - Lock-free atomic distance updates with custom CUDA primitives
   - METIS-integrated graph partitioning for SSSP

2. **Implementation:**
   - Production-quality C++/CUDA codebase with full annotations
   - Comprehensive benchmark framework with 9 datasets
   - Correctness validation against classical algorithms

3. **Experimental Analysis:**
   - Detailed performance characterization on HPC cluster
   - Communication overhead analysis with edge-cut metrics
   - Scalability study (1-8 GPUs)

4. **Practical Insights:**
   - Best practices for multi-GPU graph algorithm implementation
   - Trade-offs between partitioning quality and overhead
   - Applicability analysis for different graph types

---

## 5. Paper Structure Outline
## 5. ë…¼ë¬¸ êµ¬ì¡° ê°œìš”

### English Version

**Title:** Breaking the Sorting Barrier in Practice: Multi-GPU Acceleration of O(m log^(2/3) n) SSSP with Asynchronous Pipeline

**Sections:**

1. **Introduction** (2 pages)
   - Background on SSSP and historical O(m + n log n) barrier
   - Recent theoretical breakthrough (Duan et al. 2025)
   - Motivation for HPC optimization
   - Paper contributions and structure

2. **Related Work** (1.5 pages)
   - Classical algorithms: Dijkstra, Bellman-Ford
   - Parallel SSSP: Î”-stepping, GraphLab
   - GPU implementations: Gunrock, CuSha
   - Multi-GPU techniques: NVLINK, CUDA-aware MPI

3. **Background and Preliminaries** (2 pages)
   - Graph definitions and notation
   - Duan et al. algorithm overview (Algorithms 1-3)
   - Time complexity proof sketch
   - HPC architecture: NVLINK, GPU memory hierarchy

4. **Proposed Technique: MGAP** (3 pages)
   - Architecture overview with diagrams
   - Component 1: NVLINK-based multi-GPU coordination
   - Component 2: Asynchronous pipeline design
   - Component 3: METIS graph partitioning
   - Component 4: Lock-free atomic operations
   - Theoretical analysis: complexity and communication cost

5. **Implementation** (2.5 pages)
   - Software architecture
   - CUDA kernel design
   - Memory management strategy
   - Synchronization mechanisms
   - Code annotations and best practices

6. **Experimental Evaluation** (4 pages)
   - Experimental setup (hardware, datasets)
   - Correctness validation
   - Performance results: time, speedup, throughput
   - Communication analysis: edge-cut, volume, overhead
   - Scalability study
   - Ablation study: impact of each component

7. **Discussion** (1.5 pages)
   - Strengths: performance gains, scalability
   - Weaknesses: partitioning overhead, memory constraints
   - Applicability: graph types, problem sizes
   - Ease of deployment

8. **Conclusion** (1 page)
   - Summary of contributions
   - Practical impact
   - Future work: dynamic graphs, distributed clusters

9. **References** (1 page)

**Total:** ~18-20 pages (excluding references and appendix)

### Korean Version (í•œêµ­ì–´ ë²„ì „)

**ì œëª©:** ì •ë ¬ ì¥ë²½ì„ ì‹¤ì „ì—ì„œ ëŒíŒŒí•˜ê¸°: ë¹„ë™ê¸° íŒŒì´í”„ë¼ì¸ì„ í™œìš©í•œ O(m log^(2/3) n) SSSPì˜ Multi-GPU ê°€ì†

**ì„¹ì…˜:**

1. **ì„œë¡ ** (2í˜ì´ì§€)
2. **ê´€ë ¨ ì—°êµ¬** (1.5í˜ì´ì§€)
3. **ë°°ê²½ ë° ì˜ˆë¹„ ì§€ì‹** (2í˜ì´ì§€)
4. **ì œì•ˆ ê¸°ë²•: MGAP** (3í˜ì´ì§€)
5. **êµ¬í˜„** (2.5í˜ì´ì§€)
6. **ì‹¤í—˜ í‰ê°€** (4í˜ì´ì§€)
7. **ë…¼ì˜** (1.5í˜ì´ì§€)
8. **ê²°ë¡ ** (1í˜ì´ì§€)
9. **ì°¸ê³ ë¬¸í—Œ** (1í˜ì´ì§€)

---

## 6. Implementation Timeline
## 6. êµ¬í˜„ ì¼ì •

### Phase 1: Baseline Enhancement (Days 1-3)
- [ ] Review and annotate existing Dijkstra/Bellman-Ford implementations
- [ ] Add comprehensive Korean/English comments
- [ ] Verify correctness on all test cases

### Phase 2: MGAP Core Implementation (Days 4-7)
- [ ] Implement METIS graph partitioning integration
- [ ] Develop multi-GPU coordination layer with NVLINK P2P
- [ ] Create asynchronous pipeline with CUDA streams
- [ ] Implement lock-free atomicMinDouble kernel

### Phase 3: Benchmark Infrastructure (Days 8-10)
- [ ] Generate/download 9 benchmark datasets
- [ ] Extend benchmark framework with communication metrics
- [ ] Add memory profiling and GPU utilization tracking
- [ ] Create automated experiment runner scripts

### Phase 4: Experimental Evaluation (Days 11-14)
- [ ] Run correctness validation on all datasets
- [ ] Execute performance benchmarks (sequential, OpenMP, GPU, MGAP)
- [ ] Collect communication metrics (edge-cut, volume)
- [ ] Generate scalability data (1-8 GPUs if available)
- [ ] Create visualization scripts for graphs and tables

### Phase 5: Paper Writing (Days 15-21)
- [ ] Write English version (sections 1-9)
- [ ] Create figures and tables
- [ ] Translate to Korean
- [ ] Generate PDF using LaTeX
- [ ] Create Word-exportable version (Pandoc)

### Phase 6: Verification and Finalization (Days 22-25)
- [ ] Code-paper consistency check
- [ ] Reproducibility verification
- [ ] Peer review simulation
- [ ] Final polishing and formatting

---

## 7. Validation Checklist
## 7. ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Code Implementation Checklist

- [ ] **Correctness:**
  - [ ] All unit tests pass (sequential, parallel, CUDA)
  - [ ] Distance accuracy within 1e-5 tolerance
  - [ ] Path reconstruction verified
  - [ ] Disconnected graph handling

- [ ] **Performance:**
  - [ ] Sequential baseline matches expected O(m log^(2/3) n)
  - [ ] MGAP achieves â‰¥10Ã— speedup over sequential
  - [ ] NVLINK bandwidth â‰¥300 GB/s measured
  - [ ] Communication volume <40% of single-GPU

- [ ] **Scalability:**
  - [ ] Linear speedup for 1â†’2 GPUs (â‰¥1.7Ã—)
  - [ ] Sub-linear but positive for 2â†’4 GPUs (â‰¥1.4Ã—)
  - [ ] Weak scaling efficiency â‰¥70%

- [ ] **Code Quality:**
  - [ ] Korean + English annotations for all functions
  - [ ] Memory leak-free (verified with cuda-memcheck)
  - [ ] Proper error handling (CUDA_CHECK macros)
  - [ ] Compilation warnings resolved

### Paper Quality Checklist

- [ ] **Content:**
  - [ ] All claims supported by experimental data
  - [ ] Complexity analysis mathematically sound
  - [ ] Related work comprehensive and accurate
  - [ ] Figures/tables have clear captions

- [ ] **Consistency:**
  - [ ] Algorithm pseudocode matches implementation
  - [ ] Performance numbers match benchmark outputs
  - [ ] Graph sizes consistent across tables
  - [ ] References properly formatted

- [ ] **Language:**
  - [ ] English version grammatically correct
  - [ ] Korean version professionally translated
  - [ ] Technical terms consistently used
  - [ ] Abstract <250 words

- [ ] **Formatting:**
  - [ ] PDF renders correctly (fonts, equations)
  - [ ] Word export preserves formatting
  - [ ] Figures high-resolution (â‰¥300 DPI)
  - [ ] Code listings syntax-highlighted

### Reproducibility Checklist

- [ ] **Documentation:**
  - [ ] README with build instructions
  - [ ] Dataset download links provided
  - [ ] Benchmark scripts included
  - [ ] Hardware requirements specified

- [ ] **Artifacts:**
  - [ ] Source code on GitHub with commit hash
  - [ ] Datasets archived (or generation scripts)
  - [ ] Benchmark results in CSV format
  - [ ] Visualization scripts for plots

---

## 8. Success Criteria
## 8. ì„±ê³µ ê¸°ì¤€

**Minimum Viable Paper (MVP):**
1. âœ… MGAP implementation compiles and runs correctly
2. âœ… Achieves â‰¥10Ã— speedup over sequential baseline
3. âœ… Reduces communication volume by â‰¥30%
4. âœ… Complete paper draft in English and Korean (15+ pages)
5. âœ… All experiments reproducible with provided scripts

**Stretch Goals:**
- ğŸ¯ Achieve 50Ã— speedup on billion-edge graphs
- ğŸ¯ Strong scaling efficiency >80% (1â†’4 GPUs)
- ğŸ¯ Published dataset contributions (new benchmark suite)
- ğŸ¯ Acceptance at domestic conference (KCC, KSC)

---

## 9. References (Preliminary)
## 9. ì°¸ê³ ë¬¸í—Œ (ì˜ˆë¹„)

1. **Duan et al.** (2025). "Breaking the Sorting Barrier for Directed Single-Source Shortest Paths." arXiv:2504.17033v2.

2. **Dijkstra, E. W.** (1959). "A note on two problems in connexion with graphs." Numerische mathematik, 1(1), 269-271.

3. **Bellman, R.** (1958). "On a routing problem." Quarterly of applied mathematics, 16(1), 87-90.

4. **Meyer, U., & Sanders, P.** (2003). "Î”-stepping: a parallelizable shortest path algorithm." Journal of Algorithms, 49(1), 114-152.

5. **Wang, Y., et al.** (2017). "Gunrock: A high-performance graph processing library on the GPU." ACM SIGPLAN Notices, 52(8), 265-266.

6. **Karypis, G., & Kumar, V.** (1998). "A fast and high quality multilevel scheme for partitioning irregular graphs." SIAM Journal on scientific Computing, 20(1), 359-392.

7. **NVIDIA Corporation.** (2023). "NVLINK and NVSwitch Architecture Whitepaper."

8. **Besta, M., et al.** (2019). "Slim Graph: Practical Lossy Graph Compression for Approximate Graph Processing, Storage, and Analytics." SC19.

---

## Appendix A: Detailed Algorithm Pseudocode
## ë¶€ë¡ A: ìƒì„¸ ì•Œê³ ë¦¬ì¦˜ ì˜ì‚¬ì½”ë“œ

```cpp
// MGAP-SSSP Detailed Implementation

// Phase 1: Graph Partitioning
void partition_graph(Graph& G, int k_gpus) {
    // Use METIS k-way partitioning
    idx_t nvtxs = G.n;
    idx_t ncon = 1;  // Number of constraints
    idx_t nparts = k_gpus;
    idx_t objval;  // Edge-cut value

    idx_t options[METIS_NOPTIONS];
    METIS_SetDefaultOptions(options);
    options[METIS_OPTION_OBJTYPE] = METIS_OBJTYPE_CUT;  // Minimize edge-cut
    options[METIS_OPTION_NUMBERING] = 0;  // C-style numbering

    // Partition graph
    int ret = METIS_PartGraphKway(
        &nvtxs, &ncon, G.xadj, G.adjncy,
        NULL, NULL, NULL,  // Vertex weights, sizes, edge weights
        &nparts, NULL, NULL,  // Target partition weights, ubvec
        options, &objval, G.partition
    );

    // Distribute partitions to GPUs
    for (int gpu_id = 0; gpu_id < k_gpus; ++gpu_id) {
        cudaSetDevice(gpu_id);
        build_local_graph(G, gpu_id);
        identify_boundary_vertices(G, gpu_id);
    }
}

// Phase 2: Multi-GPU SSSP with Asynchronous Pipeline
void mgap_sssp(Graph G[], int k_gpus, vertex_t source) {
    // Initialize CUDA streams for each GPU
    cudaStream_t compute_streams[k_gpus];
    cudaStream_t transfer_streams[k_gpus];
    cudaEvent_t events[k_gpus];

    for (int i = 0; i < k_gpus; ++i) {
        cudaSetDevice(i);
        cudaStreamCreate(&compute_streams[i]);
        cudaStreamCreate(&transfer_streams[i]);
        cudaEventCreate(&events[i]);

        // Enable P2P access to all other GPUs
        for (int j = 0; j < k_gpus; ++j) {
            if (i != j) cudaDeviceEnablePeerAccess(j, 0);
        }
    }

    // Initialize distances
    for (int i = 0; i < k_gpus; ++i) {
        cudaSetDevice(i);
        initialize_distances<<<blocks, threads, 0, compute_streams[i]>>>(
            G[i].d_distances, G[i].n, source
        );
    }

    bool global_changed = true;
    int iteration = 0;

    // Main iteration loop
    while (global_changed && iteration < MAX_ITER) {
        global_changed = false;

        // Stage 1: Local relaxation (parallel across GPUs)
        for (int i = 0; i < k_gpus; ++i) {
            cudaSetDevice(i);

            // Launch edge relaxation kernel
            relax_edges_kernel<<<blocks, threads, 0, compute_streams[i]>>>(
                G[i].d_row_offsets,
                G[i].d_col_indices,
                G[i].d_weights,
                G[i].d_distances,
                G[i].d_changed,
                G[i].n,
                G[i].m
            );

            // Record event for synchronization
            cudaEventRecord(events[i], compute_streams[i]);
        }

        // Stage 2: Boundary exchange (asynchronous)
        for (int i = 0; i < k_gpus; ++i) {
            cudaSetDevice(i);

            // Wait for local computation to finish
            cudaStreamWaitEvent(transfer_streams[i], events[i], 0);

            // Async transfer boundary distances to neighbors
            for (int j = 0; j < k_gpus; ++j) {
                if (i == j) continue;

                // Direct P2P copy via NVLINK (non-blocking)
                cudaMemcpyPeerAsync(
                    G[j].d_boundary_distances,  // Destination
                    j,                           // Destination device
                    G[i].d_boundary_distances,  // Source
                    i,                           // Source device
                    boundary_size * sizeof(weight_t),
                    transfer_streams[i]
                );
            }
        }

        // Stage 3: Check convergence
        for (int i = 0; i < k_gpus; ++i) {
            cudaSetDevice(i);
            cudaStreamSynchronize(compute_streams[i]);
            cudaStreamSynchronize(transfer_streams[i]);

            bool local_changed;
            cudaMemcpy(&local_changed, G[i].d_changed, sizeof(bool),
                       cudaMemcpyDeviceToHost);
            global_changed |= local_changed;
        }

        iteration++;
    }

    // Cleanup
    for (int i = 0; i < k_gpus; ++i) {
        cudaSetDevice(i);
        cudaStreamDestroy(compute_streams[i]);
        cudaStreamDestroy(transfer_streams[i]);
        cudaEventDestroy(events[i]);
    }
}

// Custom atomic operation for double precision
__device__ void atomicMinDouble(double* address, double val) {
    unsigned long long* address_as_ull = (unsigned long long*)address;
    unsigned long long old = *address_as_ull;
    unsigned long long assumed;

    do {
        assumed = old;
        old = atomicCAS(
            address_as_ull,
            assumed,
            __double_as_longlong(min(val, __longlong_as_double(assumed)))
        );
    } while (assumed != old);
}

// Edge relaxation kernel
__global__ void relax_edges_kernel(
    const uint32_t* row_offsets,
    const uint32_t* col_indices,
    const double* weights,
    double* distances,
    bool* changed,
    uint32_t n,
    uint64_t m
) {
    uint32_t tid = blockIdx.x * blockDim.x + threadIdx.x;

    if (tid >= n) return;

    double d_u = distances[tid];
    if (d_u == INFINITY) return;

    // Relax all outgoing edges from vertex tid
    for (uint64_t edge = row_offsets[tid]; edge < row_offsets[tid + 1]; ++edge) {
        uint32_t v = col_indices[edge];
        double w = weights[edge];
        double new_dist = d_u + w;

        // Atomic update with custom double atomic
        double old_dist = distances[v];
        if (new_dist < old_dist) {
            atomicMinDouble(&distances[v], new_dist);
            *changed = true;
        }
    }
}
```

---

**End of Research Proposal**
**ì—°êµ¬ ì œì•ˆì„œ ì¢…ë£Œ**

---

**Next Steps:**
1. Review and approve this proposal
2. Proceed with implementation (Phases 1-6)
3. Continuous verification at each milestone
4. Deliver complete paper package (code + datasets + paper + reproducibility scripts)

**ë‹¤ìŒ ë‹¨ê³„:**
1. ë³¸ ì œì•ˆì„œ ê²€í†  ë° ìŠ¹ì¸
2. êµ¬í˜„ ì§„í–‰ (1-6ë‹¨ê³„)
3. ê° ë§ˆì¼ìŠ¤í†¤ì—ì„œ ì§€ì†ì  ê²€ì¦
4. ì™„ì „í•œ ë…¼ë¬¸ íŒ¨í‚¤ì§€ ì „ë‹¬ (ì½”ë“œ + ë°ì´í„°ì…‹ + ë…¼ë¬¸ + ì¬í˜„ì„± ìŠ¤í¬ë¦½íŠ¸)
